import torch
import ultralytics
import torch.optim as optim
from torch_lr_finder import LRFinder

from ultralytics.data.split import autosplit


def configure_optimizer(model, lr=1e-4, lr_multiplier=10, weight_decay=0.05):
    """é…ç½®åˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
    pretrained_params = []  # éª¨å¹²ç½‘ç»œï¼ˆé¢„è®­ç»ƒéƒ¨åˆ†ï¼‰
    attention_params = []  # SEAttention æ¨¡å—
    head_params = []  # æ£€æµ‹å¤´ï¼ˆheadï¼‰

    for name, param in model.named_parameters():
        if "SEAttention" in name:  # æ³¨æ„åŠ›æ¨¡å—
            attention_params.append(param)
        elif "backbone" in name:  # éª¨å¹²ç½‘ç»œ
            pretrained_params.append(param)
        else:  # æ£€æµ‹å¤´
            head_params.append(param)

    optimizer = optim.AdamW(
        [
            {"params": pretrained_params, "lr": lr},
            {"params": attention_params, "lr": lr * (lr_multiplier ** 0.5)},
            {"params": head_params, "lr": lr * lr_multiplier},
        ],
        weight_decay=weight_decay,
    )
    return optimizer


def find_optimal_lr(model, train_loader, loss_fn, lr_multiplier=10):
    """å­¦ä¹ ç‡å¯»ä¼˜ï¼ˆLR Finderï¼‰"""
    optimizer = configure_optimizer(model, lr=1e-7, lr_multiplier=lr_multiplier)
    lr_finder = LRFinder(model, optimizer, loss_fn, device="cuda" if torch.cuda.is_available() else "cpu")
    lr_finder.range_test(train_loader, end_lr=10, num_iter=100, step_mode="exp")
    _, suggested_lr = lr_finder.plot()
    lr_finder.reset()
    return suggested_lr

class SchedulerCallback:
    def __init__(self, scheduler):
        self.scheduler = scheduler

    def on_train_epoch_end(self, trainer):
        self.scheduler.step()
def train_with_lr_finder(model, data_yaml, config_path, pretrained_weights, epochs=100):
    """åˆ†é˜¶æ®µè®­ç»ƒ + å­¦ä¹ ç‡å¯»ä¼˜ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    # é˜¶æ®µ1ï¼šå†»ç»“éª¨å¹²ï¼Œå›ºå®šå­¦ä¹ ç‡
    print("ğŸš€ é˜¶æ®µ1ï¼šå†»ç»“éª¨å¹²ï¼Œè®­ç»ƒæ³¨æ„åŠ›+æ£€æµ‹å¤´")
    for name, param in model.model.named_parameters():
        if "backbone" in name and "SEAttention" not in name:
            param.requires_grad = False

    model.train(
        cfg=config_path,
        data=data_yaml,
        epochs=5,
        lr0=1e-3,  # æ˜¾å¼è®¾ç½®å­¦ä¹ ç‡
        name="SEAtt_train/phase1"
    )

    # é˜¶æ®µ2ï¼šè§£å†»éª¨å¹²ï¼Œå­¦ä¹ ç‡å¯»ä¼˜
    print("ğŸ” é˜¶æ®µ2ï¼šè§£å†»éª¨å¹²ï¼Œå­¦ä¹ ç‡å¯»ä¼˜")
    for param in model.model.parameters():
        param.requires_grad = True

    train_loader = model.get_dataloader(data_yaml, batch_size=model.args.batch)
    loss_fn = model.criterion

    # å­¦ä¹ ç‡å¯»ä¼˜ï¼ˆæ›´å®‰å…¨çš„èŒƒå›´ï¼‰
    suggested_lr = find_optimal_lr(model.model, train_loader, loss_fn)
    print(f"âœ… å»ºè®®å­¦ä¹ ç‡ï¼š{suggested_lr:.2e}")

    # é…ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = configure_optimizer(
        model.model,
        lr=suggested_lr,
        lr_multiplier=10,
        weight_decay=0.05
    )
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=7,
        T_mult=2,
        eta_min=max(1e-6, suggested_lr/10)
    )

    model.add_callback('on_train_epoch_end', lambda trainer: scheduler.step())

    # é˜¶æ®µ2è®­ç»ƒ
    print("ğŸš€ é˜¶æ®µ2ï¼šä½¿ç”¨ä¼˜åŒ–åçš„å­¦ä¹ ç‡è®­ç»ƒ")
    model.train(
        cfg=config_path,
        data=data_yaml,
        epochs=epochs,
        optimizer=optimizer,  # ç¦ç”¨YOLOé»˜è®¤ä¼˜åŒ–å™¨
        name="SEAtt_train/phase2"
    )


if __name__ == '__main__':
    autosplit('data/datasets/waste_classification/images', weights=(0.99, 0.01, 0),
              annotated_only=True)

    # é…ç½®å‚æ•°
    config_path = 'cfg/train.yaml'  # è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„
    data_yaml = 'data/datasets/waste_classification/waste_classification.yaml'  # æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
    pretrained_weights = 'weights/yolov8n_pretrained.pt'  # é¢„è®­ç»ƒæƒé‡è·¯å¾„

    # åŠ è½½æ¨¡å‹
    model = ultralytics.YOLO("cfg/models/v8/SEAtt_yolov8.yaml").load(pretrained_weights)

    # å¼€å§‹è®­ç»ƒ
    train_with_lr_finder(
        model=model,
        data_yaml=data_yaml,
        config_path=config_path,
        pretrained_weights=pretrained_weights,
        epochs=100
    )